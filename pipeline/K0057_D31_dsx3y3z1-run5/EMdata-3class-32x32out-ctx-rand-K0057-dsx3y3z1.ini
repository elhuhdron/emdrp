
# File is meant to be read with configobj python module.
# http://www.voidspace.org.uk/python/articles/configobj.shtml
# 
# See python_util/parseEMdataspec.ini for types / defaults / valid options
# NOTE: inline comments are OK in this file (but not in specification file)
# 
# Created on Jul 20, 2015, pwatkins

##############################
# Variables
#   Do not define any names that conflict with option names.
##############################

##############################
# Required options
##############################

# Input image EM data (hdf5)
imagesrc            = /mnt/syn/datasets/raw/K0057_D31_dsx3y3z1.h5       
#imagesrc            = /Data/datasets/raw/K0057_D31_dsx3y3z1.h5       

# Input segmented labels (hdf5)
labelsrc            = /mnt/syn/datasets/labels/gt/K0057_D31_dsx3y3z1_labels.h5
#labelsrc            = /Data/datasets/labels/gt/K0057_D31_dsx3y3z1_labels.h5

# Path to write hdf5 inputs and outputs
outpath             = /Data/pwatkins/stupid

# XYZ zero-based coordinates to corner chunk to use for random batches
chunk_rand          = 0,0,0

# XYZ zero-based coordinates to corner chunk to use for tiled batches
chunk_tiled         = 0,0,0    

##############################
# Optional options
##############################

# Debugging output
verbose             = False

# Variable name of the dataset (EM hdf5 file)
dataset             = data_mag_x3y3z1

# Variable name of user's labels (labels hdf5 file)
username            = labels

# For loading augmented data, default no augmented data
#augsrc              = /Data/datasets/warps/M0007_33_warps.h5

# datasets for loading augmented data, one per augmented data cube
#aug_datasets        = warpx,warpy

# Product is num_cases_per_batch (*tile_size must be factor of size_rand)
#tile_size           = 4,4,128   # 128x128x128 cube size 32 out
#tile_size           = 8,8,32, 8,1,256, 1,8,256   # 256x256x32 cube size 32 out
#tile_size           = 4,8,32   # 128x256x32 cube size 32 out
tile_size           = 6,10,96   # 128x256x32 plus 32 context == 192x320x96 cube size 32 out

# Size (xy) of the image examples to be presented to convnet (xxx - odd if not affin2?)
image_size          = 128

# Number of zslices in examples to be presented to convnet (each case), must be 1 or 3
nzslices            = 1

# Augmentations mask to use for randomized examples (== 3 for xy reflections only)
augs_mask           = 7

# Label priors to use for randomized examples (default uniform), MEM, ICS, ECS
#label_priors        = 0.3333334,0.3333333,0.3333333

# How labels are selected / balanced for network presentation
select_label_type  = ICS_ECS_MEM

# How input labels are to be interpreted by the network
label_type          = ICSorECSorMEM

# specify ECS label explicitly
ECS_label           = 1

# specify the data type for the labels (numpy unsigned integer types as string)
cubeLblTypeStr      = uint16

# The order of the xyz dimensions as a string, the last dim becomes the new z dim
#dim_ordering        = 'xyz'

# Size (xy) of the number of output pixels, *image_out_size must be a factor of size_rand
image_out_size      = 32

# Specify that hdf5 input files are stored in C-order
hdf5_Corder         = False        

# Specify that chunk indices are centered on origin (0,0,0)
origin_chunk_inds   = False    

# Whether to generate winner-take all (softmax) or independent label types
# independent labels are required for multiple pixel outputs
independent_labels  = True 

# Do not generate label lookup for generation of random batches (for test / features only, saves some time on init)
no_label_lookup     = True

# Do not generate labels at all (used for autoencoders)
no_labels           = False

# Return all zeros labels to convnet (for large feature dumps where convnet still expecting labels)
zero_labels         = False

##############################
# Optional options for "chunklist" or "chunkrange" modes versus regular mode (rand / tiled)
##############################

# Offset in chunk_rand to use for random batches
offset_rand         = 0,0,0       

# Offset in chunk_tiled to use for appending tiled batches
offset_tiled        = 0,0,0

#declare -a sizes=('256 256 128' '256 256 128' '256 256 128' '128 256 128' '256 256 32' '256 256 32' '256 256 32')
#declare -a chunks=("6 23 2" "16 19 15" "4 35 2" "4 11 14" "24 14 8" "13 18 15" "10 11 18")
#declare -a offsets=("0 0 32" "0 0 32" "96 96 96"  "96 64 112" "0 0 0" "0 0 64" "32 96 48")

# Set these to use "chunklist" mode which loads cubes on-the-fly from different parts of the dataset.
chunk_range_beg     = 6,23,2, 6,23,2, 6,23,2, 6,23,2, 6,23,2, 6,23,2, 6,23,2, 6,23,2, 16,19,15, 16,19,15, 16,19,15, 16,19,15, 16,19,15, 16,19,15, 16,19,15, 16,19,15, 4,35,2, 4,35,2, 4,35,2, 4,35,2, 4,35,2, 4,35,2, 4,35,2, 4,35,2, 4,11,14, 4,11,14, 4,11,14, 4,11,14, 24,14,8, 24,14,8, 13,18,15, 13,18,15, 10,11,18, 10,11,18

# chunk_range_beg is set and end range is empty, then chunk_range_beg are assumed to be single chunks
#chunk_range_end     = 23,27,7, 28,25,6

# define offsets, if set for "chunk list mode uses ranges and this list is empty, defaults to all zeros
#offset_list         = 0,0,32, 0,0,64, 0,0,96, 0,0,128, 128,0,32, 128,0,64, 128,0,96, 128,0,128, 0,0,32, 0,0,64, 0,0,96, 0,0,128, 128,0,32, 128,0,64, 128,0,96, 128,0,128, 96,96,96, 96,96,128, 96,96,160, 96,96,192, 224,96,96, 224,96,128, 224,96,160, 224,96,192, 96,64,112, 96,64,144, 96,64,176, 96,64,208, 0,0,0, 128,0,0, 0,0,64, 128,0,64, 32,96,48, 160,96,48
offset_list         = -32,-32,0, -32,-32,32, -32,-32,64, -32,-32,96, 96,-32,0, 96,-32,32, 96,-32,64, 96,-32,96, -32,-32,0, -32,-32,32, -32,-32,64, -32,-32,96, 96,-32,0, 96,-32,32, 96,-32,64, 96,-32,96, 64,64,64, 64,64,96, 64,64,128, 64,64,160, 192,64,64, 192,64,96, 192,64,128, 192,64,160, 64,32,80, 64,32,112, 64,32,144, 64,32,176, -32,-32,-32, 96,-32,-32, -32,-32,32, 96,-32,32, 0,64,16, 128,64,16

# this is used along with skip list so that tiled batches only select from the chunk_skip_list,
#   which in this case would be the test chunks in the cross validation.
chunk_skip_is_test  = True

# if chunklist or chunkrange mode, whether to load all chunks into system memory at once
chunk_list_all      = True

##############################
# Optional options that get re-ordered on reslice
##############################

# Size of voxels to sample over for random batches
#size_rand           = 256,256,32
#size_rand           = 128,128,128
#size_rand           = 128,256,32
size_rand           = 192,320,96

# Number of zslices to append for tiled batches. Default to Z value of rand_size (after reslice re-ordered)
# Use nz_tiled of zero to not load any tiled batches (for no test slices or chunklist mode)
nz_tiled            = 0

# Frontend "read_size" area for a single set of dense labels. Default to rand_size
read_size           = -1,-1,-1

# Use along with read_size to specify border around read_size cubes not to select rand examples from
read_border         = 0,0,0

##############################
# Optional data pre-processing options
##############################

# Scalar mean value to subtract from image examples (< 0 for mean over all input data, 0 to do nothing)
# estimates based on --volume_range_beg 4 9 1 --volume_range_end 49 37 19 --cube_size 3 4 6
EM_mean             = 155.5
#EM_mean             = -1.0
#EM_mean             = 0.0

# Scalar std value to divide out of image examples (< 1 for std over all input data, 1 to do nothing)
EM_std              = 35.5
#EM_std              = -1.0
#EM_std              = 1.0

# Scalar means to subtract from augmented data (< 0 for mean over all input data, 0 to do nothing)
#aug_mean            = 1.075,0.913

# Scalar std value to divide out of image examples (< 0 for std over all input data, 1 to do nothing)
#aug_std             = 1.686,1.362

